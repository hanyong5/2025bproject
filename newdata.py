import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import random
import re
from datetime import datetime, timedelta
import json
import os
import hashlib
import glob

URL = "https://finance.naver.com/news/mainnews.naver"

user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/117.0",
]

accept_languages = [
    "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "ko,en-US;q=0.9,en;q=0.8",
    "en-US,en;q=0.9,ko-KR;q=0.8,ko;q=0.7"
]

headers = {
    "User-Agent": random.choice(user_agents),
    "Accept-Language": random.choice(accept_languages),
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Referer": "https://finance.naver.com/",
    "Cache-Control": "no-cache",
}

# ì˜¤ëŠ˜ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
def get_today_date():
    """ì˜¤ëŠ˜ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    return datetime.now().strftime('%Y-%m-%d')

# ë‚ ì§œì™€ í˜ì´ì§€ë¥¼ í¬í•¨í•œ URL ìƒì„±
def build_url_with_params(date=None, page=1):
    """ë‚ ì§œì™€ í˜ì´ì§€ íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•œ URL ìƒì„±"""
    if date is None:
        date = get_today_date()
    
    return f"{URL}?date={date}&page={page}"

# ë°ì´í„° í•´ì‹œê°’ ê³„ì‚° (ì¤‘ë³µ ì²´í¬ìš©)
def calculate_data_hash(data):
    """ë°ì´í„°ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜"""
    data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(data_str.encode('utf-8')).hexdigest()

# ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
def load_existing_data(filepath):
    """ê¸°ì¡´ JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    if os.path.exists(filepath):
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return data
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    return None

# ë°ì´í„° ì €ì¥
def save_data_to_json(data, filepath):
    """ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        # data í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        output_data = {
            "date": get_today_date(),
            "timestamp": datetime.now().isoformat(),
            "total_count": len(data),
            "data_hash": calculate_data_hash(data),
            "news": data
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        return True
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# ë°ì´í„° ì¤‘ë³µ ì²´í¬
def is_data_duplicate(new_data, existing_data):
    """ìƒˆ ë°ì´í„°ì™€ ê¸°ì¡´ ë°ì´í„°ê°€ ë™ì¼í•œì§€ í™•ì¸"""
    if not existing_data:
        return False
    
    # í•´ì‹œê°’ ë¹„êµ
    new_hash = calculate_data_hash(new_data)
    existing_hash = existing_data.get("data_hash", "")
    
    if new_hash == existing_hash:
        return True
    
    # í•´ì‹œê°’ì´ ì—†ê±°ë‚˜ ë‹¤ë¥¸ ê²½ìš°, ì‹¤ì œ ë°ì´í„° ë¹„êµ
    new_news = new_data
    existing_news = existing_data.get("news", [])
    
    if len(new_news) != len(existing_news):
        return False
    
    # ì œëª© ê¸°ì¤€ìœ¼ë¡œ ë¹„êµ
    new_titles = {item.get("ì œëª©", "") for item in new_news}
    existing_titles = {item.get("ì œëª©", "") for item in existing_news}
    
    return new_titles == existing_titles

# ì˜¤ëŠ˜ ë‚ ì§œì˜ ë‹¤ìŒ íŒŒì¼ ë²ˆí˜¸ ì°¾ê¸°
def get_next_file_number(data_dir, date):
    """ì˜¤ëŠ˜ ë‚ ì§œì˜ ê¸°ì¡´ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì—¬ ë‹¤ìŒ ë²ˆí˜¸ ë°˜í™˜"""
    pattern = os.path.join(data_dir, f"{date}_*.json")
    existing_files = glob.glob(pattern)
    
    if not existing_files:
        return "01"
    
    # íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ
    numbers = []
    for filepath in existing_files:
        filename = os.path.basename(filepath)
        # YYYY-MM-DD_NN.json í˜•ì‹ì—ì„œ NN ì¶”ì¶œ
        try:
            parts = filename.replace('.json', '').split('_')
            if len(parts) >= 2:
                num_str = parts[-1]
                if num_str.isdigit():
                    numbers.append(int(num_str))
        except:
            continue
    
    if not numbers:
        return "01"
    
    # ë‹¤ìŒ ë²ˆí˜¸ ê³„ì‚°
    next_num = max(numbers) + 1
    return f"{next_num:02d}"

# íŒŒì¼ëª… ìƒì„±
def generate_filename(data_dir, date):
    """ë‚ ì§œì™€ ë²ˆí˜¸ë¥¼ í¬í•¨í•œ íŒŒì¼ëª… ìƒì„±"""
    file_number = get_next_file_number(data_dir, date)
    filename = f"{date}_{file_number}.json"
    return os.path.join(data_dir, filename)

# 5ì¼ ì´ìƒ ì§€ë‚œ íŒŒì¼ ì‚­ì œ
def delete_old_files(data_dir, days=5):
    """ì§€ì •ëœ ì¼ìˆ˜ ì´ìƒ ì§€ë‚œ íŒŒì¼ ì‚­ì œ"""
    try:
        if not os.path.exists(data_dir):
            return
        
        # í˜„ì¬ ë‚ ì§œ
        today = datetime.now()
        cutoff_date = today - timedelta(days=days)
        
        # data í´ë”ì˜ ëª¨ë“  JSON íŒŒì¼ í™•ì¸
        pattern = os.path.join(data_dir, "*.json")
        files = glob.glob(pattern)
        
        deleted_count = 0
        for filepath in files:
            try:
                # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (YYYY-MM-DD_NN.json í˜•ì‹)
                filename = os.path.basename(filepath)
                date_str = filename.split('_')[0]
                
                # ë‚ ì§œ íŒŒì‹±
                file_date = datetime.strptime(date_str, '%Y-%m-%d')
                
                # 5ì¼ ì´ìƒ ì§€ë‚œ íŒŒì¼ ì‚­ì œ
                if file_date < cutoff_date:
                    os.remove(filepath)
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ: {filename}")
            except Exception as e:
                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
                continue
        
        if deleted_count > 0:
            print(f"âœ… ì´ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"â„¹ï¸ ì‚­ì œí•  ì˜¤ë˜ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# .Nnavië¥¼ í†µí•´ ì˜¤ëŠ˜ ë‚ ì§œì˜ í˜ì´ì§€ ê°œìˆ˜ íŒŒì•…
def get_today_page_count(date=None):
    """Nnavi í´ë˜ìŠ¤ë¥¼ í†µí•´ ì§€ì •ëœ ë‚ ì§œì˜ í˜ì´ì§€ ê°œìˆ˜ íŒŒì•…"""
    try:
        if date is None:
            date = get_today_date()
        
        # ì˜¤ëŠ˜ ë‚ ì§œì˜ ì²« í˜ì´ì§€ë¡œ ìš”ì²­
        url_with_date = build_url_with_params(date=date, page=1)
        response = requests.get(url_with_date, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'euc-kr'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # .Nnavi í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸°
        nnavi_elements = soup.select('.Nnavi')
        
        if not nnavi_elements:
            return 0
        
        page_numbers = []
        max_page = 0
        
        # ì˜¤ëŠ˜ ë‚ ì§œ í™•ì¸
        today = datetime.now().strftime('%Y%m%d')
        
        for nnavi_container in nnavi_elements:
            # í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ ì°¾ê¸° (ì¼ë°˜ì ìœ¼ë¡œ a íƒœê·¸ì— í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆìŒ)
            # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´: ìˆ«ìë§Œ ìˆëŠ” ë§í¬ë‚˜ í˜ì´ì§€ ë²ˆí˜¸ê°€ í¬í•¨ëœ ë§í¬
            page_links = nnavi_container.find_all('a', href=True)
            
            # "ë‹¤ìŒ", "ë§ˆì§€ë§‰", "Last" ë“±ì˜ ë²„íŠ¼ ì°¾ê¸°
            next_buttons = nnavi_container.find_all(['a', 'span', 'button'], 
                                                    string=lambda x: x and any(keyword in str(x) for keyword in ['ë‹¤ìŒ', 'ë§ˆì§€ë§‰', 'Last', 'next', '>', 'Â»']))
            
            for link in page_links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                # ë§í¬ í…ìŠ¤íŠ¸ê°€ ìˆ«ìì¸ ê²½ìš°
                if link_text.isdigit():
                    page_num = int(link_text)
                    page_numbers.append(page_num)
                    if page_num > max_page:
                        max_page = page_num
                
                # "ë§ˆì§€ë§‰", "Last" ë“±ì˜ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë§í¬ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                if any(keyword in link_text for keyword in ['ë§ˆì§€ë§‰', 'Last', 'ë']):
                    # URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
                    if href:
                        parsed_url = urlparse(href)
                        query_params = parse_qs(parsed_url.query)
                        for param in ['page', 'p', 'pageno', 'pagenum', 'pageNum']:
                            if param in query_params:
                                try:
                                    page_num = int(query_params[param][0])
                                    if page_num > max_page:
                                        max_page = page_num
                                except (ValueError, IndexError):
                                    pass
                
                # URLì— í˜ì´ì§€ ë²ˆí˜¸ê°€ í¬í•¨ëœ ê²½ìš° (ì˜ˆ: page=2, p=3 ë“±)
                if href:
                    # URL íŒŒë¼ë¯¸í„°ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
                    parsed_url = urlparse(href)
                    query_params = parse_qs(parsed_url.query)
                    
                    # ì¼ë°˜ì ì¸ í˜ì´ì§€ íŒŒë¼ë¯¸í„°ëª…ë“¤
                    page_params = ['page', 'p', 'pageno', 'pagenum', 'pageNum']
                    for param in page_params:
                        if param in query_params:
                            try:
                                page_num = int(query_params[param][0])
                                page_numbers.append(page_num)
                                if page_num > max_page:
                                    max_page = page_num
                            except (ValueError, IndexError):
                                pass
                    
                    # URL ê²½ë¡œì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸° (ì˜ˆ: /page/2, /p/3)
                    path_parts = parsed_url.path.split('/')
                    for part in path_parts:
                        if part.isdigit():
                            page_num = int(part)
                            # ë„ˆë¬´ í° ìˆ«ìëŠ” ì œì™¸ (ì—°ë„ ë“±)
                            if 1 <= page_num <= 1000:
                                page_numbers.append(page_num)
                                if page_num > max_page:
                                    max_page = page_num
            
            # í˜ì´ì§€ ë²ˆí˜¸ê°€ ì•„ë‹Œ ë‹¤ë¥¸ í˜•íƒœë¡œ í‘œì‹œëœ ê²½ìš°
            # ì˜ˆ: "1 2 3 ... 10" í˜•íƒœì˜ í…ìŠ¤íŠ¸
            text_content = nnavi_container.get_text()
            # ìˆ«ì íŒ¨í„´ ì°¾ê¸°
            number_pattern = r'\b(\d+)\b'
            numbers = re.findall(number_pattern, text_content)
            for num_str in numbers:
                try:
                    num = int(num_str)
                    # í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë³´ì´ëŠ” ë²”ìœ„ (1~1000)
                    if 1 <= num <= 1000:
                        page_numbers.append(num)
                        if num > max_page:
                            max_page = num
                except ValueError:
                    pass
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_pages = sorted(set(page_numbers))
        
        # ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ë°˜í™˜ (í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´)
        if max_page > 0:
            return max_page
        
        # í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê³ ìœ í•œ í˜ì´ì§€ ë²ˆí˜¸ ê°œìˆ˜ ë°˜í™˜
        return len(unique_pages) if unique_pages else 0
        
    except Exception as e:
        print(f"âš ï¸ í˜ì´ì§€ ê°œìˆ˜ íŒŒì•… ì¤‘ ì˜¤ë¥˜: {e}")
        return 0

# ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ë° ìš”ì•½
def fetch_news_content(link):
    """ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    try:
        if not link:
            return None
        
        response = requests.get(link, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'euc-kr'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì°¾ê¸° (ì¼ë°˜ì ì¸ êµ¬ì¡°)
        content_selectors = [
            '#articleBodyContents',
            '#articleBody',
            '.article_body',
            '.articleBody',
            '.news_end_body',
            '.article_view',
            '#newsEndContents',
            '#articleBodyContents .go_trans _article_content',
            '.article_info'
        ]
        
        content = None
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ì œê±°
                for script in content_elem(["script", "style", "iframe", "noscript"]):
                    script.decompose()
                content = content_elem.get_text(separator=' ', strip=True)
                if content and len(content) > 50:  # ìµœì†Œ 50ì ì´ìƒ
                    break
        
        # ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ì¼ë°˜ì ì¸ ë³¸ë¬¸ íƒœê·¸ ì°¾ê¸°
        if not content or len(content) < 50:
            # idì— bodyê°€ í¬í•¨ëœ div ì°¾ê¸°
            content_elem = soup.find('div', id=lambda x: x and 'body' in str(x).lower())
            if not content_elem:
                # article íƒœê·¸ ì°¾ê¸°
                content_elem = soup.find('article')
            if not content_elem:
                # classì— articleì´ í¬í•¨ëœ ìš”ì†Œ ì°¾ê¸°
                content_elem = soup.find('div', class_=lambda x: x and 'article' in str(x).lower())
            
            if content_elem:
                for script in content_elem(["script", "style", "iframe", "noscript"]):
                    script.decompose()
                content = content_elem.get_text(separator=' ', strip=True)
        
        # ë³¸ë¬¸ ì •ë¦¬ (ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ ì œê±°)
        if content:
            content = ' '.join(content.split())  # ì—°ì† ê³µë°± ì œê±°
            # ê´‘ê³ ë‚˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
            lines = content.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line and len(line) > 10:
                    # ê´‘ê³  ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¤„ ì œì™¸
                    if not any(keyword in line for keyword in ['ê´‘ê³ ', 'AD', 'Advertisement', 'ë¬´ë‹¨ì „ì¬', 'ì €ì‘ê¶Œ']):
                        cleaned_lines.append(line)
            content = ' '.join(cleaned_lines)
        
        return content if content and len(content) > 50 else None
        
    except Exception as e:
        return None

# íŠ¹ì • í˜ì´ì§€ì˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
def fetch_news_list_from_page(date=None, page=1):
    """íŠ¹ì • ë‚ ì§œì™€ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    try:
        if date is None:
            date = get_today_date()
        
        url_with_params = build_url_with_params(date=date, page=page)
        response = requests.get(url_with_params, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'euc-kr'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_items = []
        
        # .newsList í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸°
        news_list = soup.select('.newsList')
        
        if news_list:
            # ê° ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì²˜ë¦¬
            for news_container in news_list:
                # ë‰´ìŠ¤ í•­ëª©ë“¤ ì°¾ê¸° (ì¼ë°˜ì ìœ¼ë¡œ li íƒœê·¸ë‚˜ article íƒœê·¸)
                news_items_in_container = news_container.find_all(['li', 'article', 'div'], class_=lambda x: x and ('news' in x.lower() or 'item' in x.lower()))
                
                # ë§Œì•½ ì§ì ‘ì ì¸ ë‰´ìŠ¤ í•­ëª©ì´ ì—†ë‹¤ë©´, a íƒœê·¸ë¡œ ë§í¬ê°€ ìˆëŠ” í•­ëª© ì°¾ê¸°
                if not news_items_in_container:
                    news_items_in_container = news_container.find_all('a', href=True)
                
                for item in news_items_in_container:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.find(['a', 'strong', 'span', 'h3', 'h4'])
                    if not title_elem:
                        title_elem = item
                    
                    title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.find('a', href=True) if item.name != 'a' else item
                    link = urljoin(URL, link_elem['href']) if link_elem and link_elem.get('href') else None
                    
                    # ì‹œê°„/ë‚ ì§œ ì¶”ì¶œ - .articleSummary .wdate ìš°ì„  ì‚¬ìš©
                    time_elem = item.select_one('.articleSummary .wdate')
                    if not time_elem:
                        time_elem = item.find(['span', 'em', 'time'], class_=lambda x: x and ('time' in x.lower() or 'date' in x.lower() or 'wdate' in x.lower()))
                    if not time_elem:
                        time_elem = item.find('span', string=lambda x: x and any(char in str(x) for char in ['ë¶„', 'ì‹œê°„', 'ì¼', ':', '-']))
                    time_text = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # ìš”ì•½/ë‚´ìš© ì¶”ì¶œ - .articleSummary ìš°ì„  ì‚¬ìš©
                    summary_elem = item.select_one('.articleSummary')
                    if summary_elem:
                        # .wdateëŠ” ì‹œê°„ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ìš”ì•½ë§Œ ì¶”ì¶œ
                        wdate_elem = summary_elem.select_one('.wdate')
                        if wdate_elem:
                            wdate_elem.decompose()  # ì‹œê°„ ë¶€ë¶„ ì œê±°
                        summary = summary_elem.get_text(strip=True)
                    else:
                        summary_elem = item.find(['p', 'span', 'div'], class_=lambda x: x and ('summary' in x.lower() or 'desc' in x.lower() or 'content' in x.lower()))
                        summary = summary_elem.get_text(strip=True) if summary_elem else ""
                    
                    if title and title != "ì œëª© ì—†ìŒ":
                        news_items.append({
                            "ì œëª©": title,
                            "ë§í¬": link,

                        })
        else:
            # .newsListê°€ ì—†ëŠ” ê²½ìš°, ë‹¤ë¥¸ ì¼ë°˜ì ì¸ ë‰´ìŠ¤ êµ¬ì¡° ì‹œë„
            # ul.newsList ë˜ëŠ” div.newsList ë“±
            news_list_alt = soup.select('ul.newsList, div.newsList, .newsList ul, .newsList li')
            
            for item in news_list_alt:
                title_elem = item.find('a')
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    link = urljoin(URL, title_elem.get('href', ''))
                    
                    # ì‹œê°„/ë‚ ì§œ ì¶”ì¶œ - .articleSummary .wdate ìš°ì„  ì‚¬ìš©
                    time_elem = item.select_one('.articleSummary .wdate')
                    if not time_elem:
                        time_elem = item.find(['span', 'em'], class_=lambda x: x and ('time' in str(x).lower() or 'wdate' in str(x).lower()))
                    time_text = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # ìš”ì•½/ë‚´ìš© ì¶”ì¶œ - .articleSummary ìš°ì„  ì‚¬ìš©
                    summary_elem = item.select_one('.articleSummary')
                    if summary_elem:
                        # .wdateëŠ” ì‹œê°„ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ìš”ì•½ë§Œ ì¶”ì¶œ
                        wdate_elem = summary_elem.select_one('.wdate')
                        if wdate_elem:
                            wdate_elem.decompose()  # ì‹œê°„ ë¶€ë¶„ ì œê±°
                        summary = summary_elem.get_text(strip=True)
                    else:
                        summary = ""
                    
                    if title:
                        news_items.append({
                            "ì œëª©": title,
                            "ë§í¬": link,

                        })
        
        # .Nnavi í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸° (ì¶”ê°€ ë°ì´í„°)
        nnavi_elements = soup.select('.Nnavi')
        
        if nnavi_elements:
            for nnavi_container in nnavi_elements:
                # Nnavi ë‚´ë¶€ì˜ ë‰´ìŠ¤ í•­ëª©ë“¤ ì°¾ê¸°
                nnavi_items = nnavi_container.find_all(['li', 'article', 'div', 'a'], recursive=True)
                
                for item in nnavi_items:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.find(['a', 'strong', 'span', 'h3', 'h4', 'dt', 'dd'])
                    if not title_elem:
                        # ì§ì ‘ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
                        if item.name == 'a' or item.get_text(strip=True):
                            title_elem = item
                    
                    title = title_elem.get_text(strip=True) if title_elem else ""
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.find('a', href=True) if item.name != 'a' else item
                    if item.name == 'a' and item.get('href'):
                        link_elem = item
                    
                    link = None
                    if link_elem:
                        href = link_elem.get('href') if hasattr(link_elem, 'get') else (link_elem['href'] if 'href' in link_elem.attrs else None)
                        if href:
                            link = urljoin(URL, href)
                    
                    # ì‹œê°„/ë‚ ì§œ ì¶”ì¶œ - .articleSummary .wdate ìš°ì„  ì‚¬ìš©
                    time_elem = item.select_one('.articleSummary .wdate')
                    if not time_elem:
                        time_elem = item.find(['span', 'em', 'time', 'dd'], class_=lambda x: x and ('time' in x.lower() or 'date' in x.lower() or 'wdate' in x.lower()))
                    if not time_elem:
                        time_elem = item.find('span', string=lambda x: x and any(char in str(x) for char in ['ë¶„', 'ì‹œê°„', 'ì¼', ':', '-']))
                    time_text = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # ìš”ì•½/ë‚´ìš© ì¶”ì¶œ - .articleSummary ìš°ì„  ì‚¬ìš©
                    summary_elem = item.select_one('.articleSummary')
                    if summary_elem:
                        # .wdateëŠ” ì‹œê°„ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ìš”ì•½ë§Œ ì¶”ì¶œ
                        wdate_elem = summary_elem.select_one('.wdate')
                        if wdate_elem:
                            wdate_elem.decompose()  # ì‹œê°„ ë¶€ë¶„ ì œê±°
                        summary = summary_elem.get_text(strip=True)
                    else:
                        summary_elem = item.find(['p', 'span', 'div', 'dd'], class_=lambda x: x and ('summary' in x.lower() or 'desc' in x.lower() or 'content' in x.lower()))
                        if not summary_elem:
                            # Nnavi ë‚´ë¶€ì˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì°¾ê¸°
                            summary_elem = item.find(['p', 'span', 'div'], string=lambda x: x and len(str(x).strip()) > 20)
                        summary = summary_elem.get_text(strip=True) if summary_elem else ""
                    
                    # ì¤‘ë³µ ì œê±°: ì´ë¯¸ ì¶”ê°€ëœ ë‰´ìŠ¤ì™€ ì œëª©ì´ ê°™ìœ¼ë©´ ìŠ¤í‚µ
                    if title and title != "ì œëª© ì—†ìŒ" and len(title) > 5:
                        # ì¤‘ë³µ ì²´í¬
                        is_duplicate = any(existing.get("ì œëª©") == title for existing in news_items)
                        if not is_duplicate:
                            news_items.append({
                                "ì œëª©": title,
                                "ë§í¬": link,
                                "ì¶œì²˜": "Nnavi"  # Nnaviì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì„ì„ í‘œì‹œ
                            })
        
        return news_items
        
    except Exception as e:
        print(f"âš ï¸ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []

# ì˜¤ëŠ˜ ë‚ ì§œì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ëˆ„ì  ìˆ˜ì§‘
def fetch_all_pages_news(date=None):
    """ì§€ì •ëœ ë‚ ì§œì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ëˆ„ì í•˜ì—¬ ìˆ˜ì§‘"""
    if date is None:
        date = get_today_date()
    
    all_news_items = []
    seen_titles = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì œëª© ì§‘í•©
    
    # ë¨¼ì € í˜ì´ì§€ ê°œìˆ˜ íŒŒì•…
    page_count = get_today_page_count(date=date)
    
    if page_count == 0:
        # í˜ì´ì§€ ê°œìˆ˜ë¥¼ íŒŒì•…í•˜ì§€ ëª»í•œ ê²½ìš°, ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘
        page_count = 1
    
    print(f"ğŸ“„ ì´ {page_count}í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ê° í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ë°ì´í„° ìˆ˜ì§‘
    for page in range(1, page_count + 1):
        try:
            print(f"ğŸ“„ í˜ì´ì§€ {page}/{page_count} ìˆ˜ì§‘ ì¤‘...")
            
            # í˜ì´ì§€ë³„ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            page_news = fetch_news_list_from_page(date=date, page=page)
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
            for news in page_news:
                title = news.get("ì œëª©", "")
                if title and title not in seen_titles:
                    seen_titles.add(title)
                    news["í˜ì´ì§€"] = page  # ì–´ëŠ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì™”ëŠ”ì§€ í‘œì‹œ
                    all_news_items.append(news)
            
            # API í˜¸ì¶œ ì œí•œì„ ê³ ë ¤í•œ ì§§ì€ ëŒ€ê¸°
            time.sleep(0.5)
            
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    print(f"âœ… ì´ {len(all_news_items)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    
    return all_news_items

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ“° ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("="*60)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    data_dir = "data"
    today_date = get_today_date()
    print(f"ğŸ“… ë‚ ì§œ: {today_date}")
    
    # ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ (5ì¼ ì´ìƒ)
    print("\nğŸ—‘ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    delete_old_files(data_dir, days=5)
    
    # ì˜¤ëŠ˜ ë‚ ì§œì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    news_list = fetch_all_pages_news(date=today_date)
    
    if news_list:
        # ì˜¤ëŠ˜ ë‚ ì§œì˜ ëª¨ë“  íŒŒì¼ í™•ì¸í•˜ì—¬ ì¤‘ë³µ ì²´í¬
        pattern = os.path.join(data_dir, f"{today_date}_*.json")
        existing_files = sorted(glob.glob(pattern), reverse=True)
        
        # ê°€ì¥ ìµœê·¼ íŒŒì¼ê³¼ ì¤‘ë³µ ì²´í¬
        existing_data = None
        if existing_files:
            existing_data = load_existing_data(existing_files[0])
        
        # ì¤‘ë³µ ì²´í¬
        if existing_data and is_data_duplicate(news_list, existing_data):
            print(f"â„¹ï¸ ê¸°ì¡´ ë°ì´í„°ì™€ ë™ì¼í•©ë‹ˆë‹¤. ë‹¤ì‹œ ë¡œë“œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            print(f"ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {len(existing_data.get('news', []))}ê°œ ë‰´ìŠ¤")
            return
        
        # ìƒˆ íŒŒì¼ëª… ìƒì„± (ë‚ ì§œ_ë²ˆí˜¸ í˜•ì‹)
        filepath = generate_filename(data_dir, today_date)
        print(f"ğŸ“ íŒŒì¼ëª…: {os.path.basename(filepath)}")
        
        # ë°ì´í„° ì €ì¥
        if save_data_to_json(news_list, filepath):
            print(f"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ ë°ì´í„° ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print("="*60 + "\n")

if __name__ == "__main__":
    main()
