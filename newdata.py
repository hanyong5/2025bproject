import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs
import random
import re
from datetime import datetime, timedelta
import json
import os
import hashlib
import glob
from openai import OpenAI
from supabase import create_client, Client
from dotenv import load_dotenv

URL = "https://finance.naver.com/news/mainnews.naver"

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = None
if os.getenv("OPENAI_API_KEY"):
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
supabase_client: Client = None
supabase_url = os.getenv("SUPABASE_URL")
supabase_key = os.getenv("SUPABASE_KEY")

if supabase_url and supabase_key:
    try:
        supabase_client = create_client(supabase_url, supabase_key)
        print(f"âœ… Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {supabase_url[:30]}...")
    except Exception as e:
        print(f"âš ï¸ Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
else:
    if not supabase_url:
        print("âš ï¸ SUPABASE_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not supabase_key:
        print("âš ï¸ SUPABASE_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/117.0",
]

accept_languages = [
    "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    "ko,en-US;q=0.9,en;q=0.8",
    "en-US,en;q=0.9,ko-KR;q=0.8,ko;q=0.7"
]

headers = {
    "User-Agent": random.choice(user_agents),
    "Accept-Language": random.choice(accept_languages),
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Referer": "https://finance.naver.com/",
    "Cache-Control": "no-cache",
}

# ì˜¤ëŠ˜ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
def get_today_date():
    """ì˜¤ëŠ˜ ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    return datetime.now().strftime('%Y-%m-%d')

# ë‚ ì§œì™€ í˜ì´ì§€ë¥¼ í¬í•¨í•œ URL ìƒì„±
def build_url_with_params(date=None, page=1):
    """ë‚ ì§œì™€ í˜ì´ì§€ íŒŒë¼ë¯¸í„°ë¥¼ í¬í•¨í•œ URL ìƒì„±"""
    if date is None:
        date = get_today_date()
    
    return f"{URL}?date={date}&page={page}"

# ë°ì´í„° í•´ì‹œê°’ ê³„ì‚° (ì¤‘ë³µ ì²´í¬ìš©)
def calculate_data_hash(data):
    """ë°ì´í„°ì˜ í•´ì‹œê°’ì„ ê³„ì‚°í•˜ì—¬ ë°˜í™˜"""
    data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(data_str.encode('utf-8')).hexdigest()

# ë°ì´í„° ì €ì¥
def save_data_to_json(data, filepath):
    """ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        # data í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        output_data = {
            "date": get_today_date(),
            "timestamp": datetime.now().isoformat(),
            "total_count": len(data),
            "data_hash": calculate_data_hash(data),
            "news": data
        }
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
        return True
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

# ì˜¤ëŠ˜ ë‚ ì§œì˜ ë‹¤ìŒ íŒŒì¼ ë²ˆí˜¸ ì°¾ê¸°
def get_next_file_number(data_dir, date):
    """ì˜¤ëŠ˜ ë‚ ì§œì˜ ê¸°ì¡´ íŒŒì¼ë“¤ì„ í™•ì¸í•˜ì—¬ ë‹¤ìŒ ë²ˆí˜¸ ë°˜í™˜"""
    pattern = os.path.join(data_dir, f"{date}_*.json")
    existing_files = glob.glob(pattern)
    
    if not existing_files:
        return "01"
    
    # íŒŒì¼ëª…ì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ
    numbers = []
    for filepath in existing_files:
        filename = os.path.basename(filepath)
        # YYYY-MM-DD_NN.json í˜•ì‹ì—ì„œ NN ì¶”ì¶œ
        try:
            parts = filename.replace('.json', '').split('_')
            if len(parts) >= 2:
                num_str = parts[-1]
                if num_str.isdigit():
                    numbers.append(int(num_str))
        except:
            continue
    
    if not numbers:
        return "01"
    
    # ë‹¤ìŒ ë²ˆí˜¸ ê³„ì‚°
    next_num = max(numbers) + 1
    return f"{next_num:02d}"

# íŒŒì¼ëª… ìƒì„±
def generate_filename(data_dir, date):
    """ë‚ ì§œì™€ ë²ˆí˜¸ë¥¼ í¬í•¨í•œ íŒŒì¼ëª… ìƒì„±"""
    file_number = get_next_file_number(data_dir, date)
    filename = f"{date}_{file_number}.json"
    return os.path.join(data_dir, filename)

# 5ì¼ ì´ìƒ ì§€ë‚œ íŒŒì¼ ì‚­ì œ (ë‰´ìŠ¤ íŒŒì¼ë§Œ)
def delete_old_files(data_dir, days=5):
    """ì§€ì •ëœ ì¼ìˆ˜ ì´ìƒ ì§€ë‚œ ë‰´ìŠ¤ íŒŒì¼ë§Œ ì‚­ì œ (ë‹¤ë¥¸ workflowì˜ íŒŒì¼ì€ ë³´í˜¸)"""
    try:
        if not os.path.exists(data_dir):
            return
        
        # í˜„ì¬ ë‚ ì§œ
        today = datetime.now()
        today_str = today.strftime('%Y-%m-%d')
        cutoff_date = today - timedelta(days=days)
        
        # ë‰´ìŠ¤ íŒŒì¼ íŒ¨í„´ë§Œ í™•ì¸ (YYYY-MM-DD_NN.json í˜•ì‹)
        pattern = os.path.join(data_dir, "????-??-??_??.json")
        files = glob.glob(pattern)
        
        deleted_count = 0
        for filepath in files:
            try:
                filename = os.path.basename(filepath)
                
                # íŒŒì¼ëª… í˜•ì‹ í™•ì¸ (YYYY-MM-DD_NN.json)
                if not filename.count('_') == 1:
                    continue
                
                parts = filename.replace('.json', '').split('_')
                if len(parts) != 2:
                    continue
                
                date_str = parts[0]
                num_str = parts[1]
                
                # ë‚ ì§œ í˜•ì‹ í™•ì¸
                if not num_str.isdigit():
                    continue
                
                # ì˜¤ëŠ˜ ë‚ ì§œì˜ íŒŒì¼ì€ ì‚­ì œí•˜ì§€ ì•ŠìŒ
                if date_str == today_str:
                    continue
                
                # ë‚ ì§œ íŒŒì‹±
                file_date = datetime.strptime(date_str, '%Y-%m-%d')
                
                # 5ì¼ ì´ìƒ ì§€ë‚œ íŒŒì¼ë§Œ ì‚­ì œ
                if file_date < cutoff_date:
                    os.remove(filepath)
                    deleted_count += 1
                    print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ë‰´ìŠ¤ íŒŒì¼ ì‚­ì œ: {filename}")
            except (ValueError, IndexError) as e:
                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ë˜ëŠ” í˜•ì‹ ì˜¤ë¥˜ ì‹œ ìŠ¤í‚µ (ë‹¤ë¥¸ workflowì˜ íŒŒì¼ì¼ ìˆ˜ ìˆìŒ)
                continue
            except Exception as e:
                # ê¸°íƒ€ ì˜¤ë¥˜ ì‹œ ìŠ¤í‚µ
                continue
        
        if deleted_count > 0:
            print(f"âœ… ì´ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ë‰´ìŠ¤ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"â„¹ï¸ ì‚­ì œí•  ì˜¤ë˜ëœ ë‰´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# .Nnavië¥¼ í†µí•´ ì˜¤ëŠ˜ ë‚ ì§œì˜ í˜ì´ì§€ ê°œìˆ˜ íŒŒì•…
def get_today_page_count(date=None):
    """Nnavi í´ë˜ìŠ¤ë¥¼ í†µí•´ ì§€ì •ëœ ë‚ ì§œì˜ í˜ì´ì§€ ê°œìˆ˜ íŒŒì•…"""
    try:
        if date is None:
            date = get_today_date()
        
        # ì˜¤ëŠ˜ ë‚ ì§œì˜ ì²« í˜ì´ì§€ë¡œ ìš”ì²­
        url_with_date = build_url_with_params(date=date, page=1)
        response = requests.get(url_with_date, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'euc-kr'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # .Nnavi í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸°
        nnavi_elements = soup.select('.Nnavi')
        
        if not nnavi_elements:
            return 0
        
        page_numbers = []
        max_page = 0
        
        # ì˜¤ëŠ˜ ë‚ ì§œ í™•ì¸
        today = datetime.now().strftime('%Y%m%d')
        
        for nnavi_container in nnavi_elements:
            # í˜ì´ì§€ë„¤ì´ì…˜ ë§í¬ ì°¾ê¸° (ì¼ë°˜ì ìœ¼ë¡œ a íƒœê·¸ì— í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆìŒ)
            # í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´: ìˆ«ìë§Œ ìˆëŠ” ë§í¬ë‚˜ í˜ì´ì§€ ë²ˆí˜¸ê°€ í¬í•¨ëœ ë§í¬
            page_links = nnavi_container.find_all('a', href=True)
            
            # "ë‹¤ìŒ", "ë§ˆì§€ë§‰", "Last" ë“±ì˜ ë²„íŠ¼ ì°¾ê¸°
            next_buttons = nnavi_container.find_all(['a', 'span', 'button'], 
                                                    string=lambda x: x and any(keyword in str(x) for keyword in ['ë‹¤ìŒ', 'ë§ˆì§€ë§‰', 'Last', 'next', '>', 'Â»']))
            
            for link in page_links:
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                # ë§í¬ í…ìŠ¤íŠ¸ê°€ ìˆ«ìì¸ ê²½ìš°
                if link_text.isdigit():
                    page_num = int(link_text)
                    page_numbers.append(page_num)
                    if page_num > max_page:
                        max_page = page_num
                
                # "ë§ˆì§€ë§‰", "Last" ë“±ì˜ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë§í¬ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ
                if any(keyword in link_text for keyword in ['ë§ˆì§€ë§‰', 'Last', 'ë']):
                    # URLì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
                    if href:
                        parsed_url = urlparse(href)
                        query_params = parse_qs(parsed_url.query)
                        for param in ['page', 'p', 'pageno', 'pagenum', 'pageNum']:
                            if param in query_params:
                                try:
                                    page_num = int(query_params[param][0])
                                    if page_num > max_page:
                                        max_page = page_num
                                except (ValueError, IndexError):
                                    pass
                
                # URLì— í˜ì´ì§€ ë²ˆí˜¸ê°€ í¬í•¨ëœ ê²½ìš° (ì˜ˆ: page=2, p=3 ë“±)
                if href:
                    # URL íŒŒë¼ë¯¸í„°ì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°
                    parsed_url = urlparse(href)
                    query_params = parse_qs(parsed_url.query)
                    
                    # ì¼ë°˜ì ì¸ í˜ì´ì§€ íŒŒë¼ë¯¸í„°ëª…ë“¤
                    page_params = ['page', 'p', 'pageno', 'pagenum', 'pageNum']
                    for param in page_params:
                        if param in query_params:
                            try:
                                page_num = int(query_params[param][0])
                                page_numbers.append(page_num)
                                if page_num > max_page:
                                    max_page = page_num
                            except (ValueError, IndexError):
                                pass
                    
                    # URL ê²½ë¡œì—ì„œ í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸° (ì˜ˆ: /page/2, /p/3)
                    path_parts = parsed_url.path.split('/')
                    for part in path_parts:
                        if part.isdigit():
                            page_num = int(part)
                            # ë„ˆë¬´ í° ìˆ«ìëŠ” ì œì™¸ (ì—°ë„ ë“±)
                            if 1 <= page_num <= 1000:
                                page_numbers.append(page_num)
                                if page_num > max_page:
                                    max_page = page_num
            
            # í˜ì´ì§€ ë²ˆí˜¸ê°€ ì•„ë‹Œ ë‹¤ë¥¸ í˜•íƒœë¡œ í‘œì‹œëœ ê²½ìš°
            # ì˜ˆ: "1 2 3 ... 10" í˜•íƒœì˜ í…ìŠ¤íŠ¸
            text_content = nnavi_container.get_text()
            # ìˆ«ì íŒ¨í„´ ì°¾ê¸°
            number_pattern = r'\b(\d+)\b'
            numbers = re.findall(number_pattern, text_content)
            for num_str in numbers:
                try:
                    num = int(num_str)
                    # í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë³´ì´ëŠ” ë²”ìœ„ (1~1000)
                    if 1 <= num <= 1000:
                        page_numbers.append(num)
                        if num > max_page:
                            max_page = num
                except ValueError:
                    pass
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_pages = sorted(set(page_numbers))
        
        # ìµœëŒ€ í˜ì´ì§€ ë²ˆí˜¸ ë°˜í™˜ (í˜ì´ì§€ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´)
        if max_page > 0:
            return max_page
        
        # í˜ì´ì§€ ë²ˆí˜¸ê°€ ì—†ìœ¼ë©´ ê³ ìœ í•œ í˜ì´ì§€ ë²ˆí˜¸ ê°œìˆ˜ ë°˜í™˜
        return len(unique_pages) if unique_pages else 0
        
    except Exception as e:
        print(f"âš ï¸ í˜ì´ì§€ ê°œìˆ˜ íŒŒì•… ì¤‘ ì˜¤ë¥˜: {e}")
        return 0

# ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ë° ìš”ì•½
def fetch_news_content(link):
    """ë‰´ìŠ¤ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
    try:
        if not link:
            return None
        
        response = requests.get(link, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'euc-kr'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì°¾ê¸° (ì¼ë°˜ì ì¸ êµ¬ì¡°)
        content_selectors = [
            '#articleBodyContents',
            '#articleBody',
            '.article_body',
            '.articleBody',
            '.news_end_body',
            '.article_view',
            '#newsEndContents',
            '#articleBodyContents .go_trans _article_content',
            '.article_info'
        ]
        
        content = None
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ ì œê±°
                for script in content_elem(["script", "style", "iframe", "noscript"]):
                    script.decompose()
                content = content_elem.get_text(separator=' ', strip=True)
                if content and len(content) > 50:  # ìµœì†Œ 50ì ì´ìƒ
                    break
        
        # ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ì¼ë°˜ì ì¸ ë³¸ë¬¸ íƒœê·¸ ì°¾ê¸°
        if not content or len(content) < 50:
            # idì— bodyê°€ í¬í•¨ëœ div ì°¾ê¸°
            content_elem = soup.find('div', id=lambda x: x and 'body' in str(x).lower())
            if not content_elem:
                # article íƒœê·¸ ì°¾ê¸°
                content_elem = soup.find('article')
            if not content_elem:
                # classì— articleì´ í¬í•¨ëœ ìš”ì†Œ ì°¾ê¸°
                content_elem = soup.find('div', class_=lambda x: x and 'article' in str(x).lower())
            
            if content_elem:
                for script in content_elem(["script", "style", "iframe", "noscript"]):
                    script.decompose()
                content = content_elem.get_text(separator=' ', strip=True)
        
        # ë³¸ë¬¸ ì •ë¦¬ (ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸ ì œê±°)
        if content:
            content = ' '.join(content.split())  # ì—°ì† ê³µë°± ì œê±°
            # ê´‘ê³ ë‚˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
            lines = content.split('\n')
            cleaned_lines = []
            for line in lines:
                line = line.strip()
                if line and len(line) > 10:
                    # ê´‘ê³  ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì¤„ ì œì™¸
                    if not any(keyword in line for keyword in ['ê´‘ê³ ', 'AD', 'Advertisement', 'ë¬´ë‹¨ì „ì¬', 'ì €ì‘ê¶Œ']):
                        cleaned_lines.append(line)
            content = ' '.join(cleaned_lines)
        
        return content if content and len(content) > 50 else None
        
    except Exception as e:
        return None

# íŠ¹ì • í˜ì´ì§€ì˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
def fetch_news_list_from_page(date=None, page=1):
    """íŠ¹ì • ë‚ ì§œì™€ í˜ì´ì§€ì˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
    try:
        if date is None:
            date = get_today_date()
        
        url_with_params = build_url_with_params(date=date, page=page)
        response = requests.get(url_with_params, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'euc-kr'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_items = []
        
        # .newsList í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸°
        news_list = soup.select('.newsList')
        
        if news_list:
            # ê° ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆ ì²˜ë¦¬
            for news_container in news_list:
                # ë‰´ìŠ¤ í•­ëª©ë“¤ ì°¾ê¸° (ì¼ë°˜ì ìœ¼ë¡œ li íƒœê·¸ë‚˜ article íƒœê·¸)
                news_items_in_container = news_container.find_all(['li', 'article', 'div'], class_=lambda x: x and ('news' in x.lower() or 'item' in x.lower()))
                
                # ë§Œì•½ ì§ì ‘ì ì¸ ë‰´ìŠ¤ í•­ëª©ì´ ì—†ë‹¤ë©´, a íƒœê·¸ë¡œ ë§í¬ê°€ ìˆëŠ” í•­ëª© ì°¾ê¸°
                if not news_items_in_container:
                    news_items_in_container = news_container.find_all('a', href=True)
                
                for item in news_items_in_container:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.find(['a', 'strong', 'span', 'h3', 'h4'])
                    if not title_elem:
                        title_elem = item
                    
                    title = title_elem.get_text(strip=True) if title_elem else "ì œëª© ì—†ìŒ"
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.find('a', href=True) if item.name != 'a' else item
                    link = urljoin(URL, link_elem['href']) if link_elem and link_elem.get('href') else None
                    
                    # ì‹œê°„/ë‚ ì§œ ì¶”ì¶œ - .articleSummary .wdate ìš°ì„  ì‚¬ìš©
                    time_elem = item.select_one('.articleSummary .wdate')
                    if not time_elem:
                        time_elem = item.find(['span', 'em', 'time'], class_=lambda x: x and ('time' in x.lower() or 'date' in x.lower() or 'wdate' in x.lower()))
                    if not time_elem:
                        time_elem = item.find('span', string=lambda x: x and any(char in str(x) for char in ['ë¶„', 'ì‹œê°„', 'ì¼', ':', '-']))
                    time_text = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # ìš”ì•½/ë‚´ìš© ì¶”ì¶œ - .articleSummary ìš°ì„  ì‚¬ìš©
                    summary_elem = item.select_one('.articleSummary')
                    if summary_elem:
                        # .wdateëŠ” ì‹œê°„ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ìš”ì•½ë§Œ ì¶”ì¶œ
                        wdate_elem = summary_elem.select_one('.wdate')
                        if wdate_elem:
                            wdate_elem.decompose()  # ì‹œê°„ ë¶€ë¶„ ì œê±°
                        summary = summary_elem.get_text(strip=True)
                    else:
                        summary_elem = item.find(['p', 'span', 'div'], class_=lambda x: x and ('summary' in x.lower() or 'desc' in x.lower() or 'content' in x.lower()))
                        summary = summary_elem.get_text(strip=True) if summary_elem else ""
                    
                    if title and title != "ì œëª© ì—†ìŒ":
                        news_items.append({
                            "ì œëª©": title,
                            "ë§í¬": link,

                        })
        else:
            # .newsListê°€ ì—†ëŠ” ê²½ìš°, ë‹¤ë¥¸ ì¼ë°˜ì ì¸ ë‰´ìŠ¤ êµ¬ì¡° ì‹œë„
            # ul.newsList ë˜ëŠ” div.newsList ë“±
            news_list_alt = soup.select('ul.newsList, div.newsList, .newsList ul, .newsList li')
            
            for item in news_list_alt:
                title_elem = item.find('a')
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    link = urljoin(URL, title_elem.get('href', ''))
                    
                    # ì‹œê°„/ë‚ ì§œ ì¶”ì¶œ - .articleSummary .wdate ìš°ì„  ì‚¬ìš©
                    time_elem = item.select_one('.articleSummary .wdate')
                    if not time_elem:
                        time_elem = item.find(['span', 'em'], class_=lambda x: x and ('time' in str(x).lower() or 'wdate' in str(x).lower()))
                    time_text = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # ìš”ì•½/ë‚´ìš© ì¶”ì¶œ - .articleSummary ìš°ì„  ì‚¬ìš©
                    summary_elem = item.select_one('.articleSummary')
                    if summary_elem:
                        # .wdateëŠ” ì‹œê°„ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ìš”ì•½ë§Œ ì¶”ì¶œ
                        wdate_elem = summary_elem.select_one('.wdate')
                        if wdate_elem:
                            wdate_elem.decompose()  # ì‹œê°„ ë¶€ë¶„ ì œê±°
                        summary = summary_elem.get_text(strip=True)
                    else:
                        summary = ""
                    
                    if title:
                        news_items.append({
                            "ì œëª©": title,
                            "ë§í¬": link,

                        })
        
        # .Nnavi í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ìš”ì†Œ ì°¾ê¸° (ì¶”ê°€ ë°ì´í„°)
        nnavi_elements = soup.select('.Nnavi')
        
        if nnavi_elements:
            for nnavi_container in nnavi_elements:
                # Nnavi ë‚´ë¶€ì˜ ë‰´ìŠ¤ í•­ëª©ë“¤ ì°¾ê¸°
                nnavi_items = nnavi_container.find_all(['li', 'article', 'div', 'a'], recursive=True)
                
                for item in nnavi_items:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = item.find(['a', 'strong', 'span', 'h3', 'h4', 'dt', 'dd'])
                    if not title_elem:
                        # ì§ì ‘ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
                        if item.name == 'a' or item.get_text(strip=True):
                            title_elem = item
                    
                    title = title_elem.get_text(strip=True) if title_elem else ""
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = item.find('a', href=True) if item.name != 'a' else item
                    if item.name == 'a' and item.get('href'):
                        link_elem = item
                    
                    link = None
                    if link_elem:
                        href = link_elem.get('href') if hasattr(link_elem, 'get') else (link_elem['href'] if 'href' in link_elem.attrs else None)
                        if href:
                            link = urljoin(URL, href)
                    
                    # ì‹œê°„/ë‚ ì§œ ì¶”ì¶œ - .articleSummary .wdate ìš°ì„  ì‚¬ìš©
                    time_elem = item.select_one('.articleSummary .wdate')
                    if not time_elem:
                        time_elem = item.find(['span', 'em', 'time', 'dd'], class_=lambda x: x and ('time' in x.lower() or 'date' in x.lower() or 'wdate' in x.lower()))
                    if not time_elem:
                        time_elem = item.find('span', string=lambda x: x and any(char in str(x) for char in ['ë¶„', 'ì‹œê°„', 'ì¼', ':', '-']))
                    time_text = time_elem.get_text(strip=True) if time_elem else ""
                    
                    # ìš”ì•½/ë‚´ìš© ì¶”ì¶œ - .articleSummary ìš°ì„  ì‚¬ìš©
                    summary_elem = item.select_one('.articleSummary')
                    if summary_elem:
                        # .wdateëŠ” ì‹œê°„ì´ë¯€ë¡œ ì œì™¸í•˜ê³  ìš”ì•½ë§Œ ì¶”ì¶œ
                        wdate_elem = summary_elem.select_one('.wdate')
                        if wdate_elem:
                            wdate_elem.decompose()  # ì‹œê°„ ë¶€ë¶„ ì œê±°
                        summary = summary_elem.get_text(strip=True)
                    else:
                        summary_elem = item.find(['p', 'span', 'div', 'dd'], class_=lambda x: x and ('summary' in x.lower() or 'desc' in x.lower() or 'content' in x.lower()))
                        if not summary_elem:
                            # Nnavi ë‚´ë¶€ì˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì°¾ê¸°
                            summary_elem = item.find(['p', 'span', 'div'], string=lambda x: x and len(str(x).strip()) > 20)
                        summary = summary_elem.get_text(strip=True) if summary_elem else ""
                    
                    # ì¤‘ë³µ ì œê±°: ì´ë¯¸ ì¶”ê°€ëœ ë‰´ìŠ¤ì™€ ì œëª©ì´ ê°™ìœ¼ë©´ ìŠ¤í‚µ
                    if title and title != "ì œëª© ì—†ìŒ" and len(title) > 5:
                        # ì¤‘ë³µ ì²´í¬
                        is_duplicate = any(existing.get("ì œëª©") == title for existing in news_items)
                        if not is_duplicate:
                            news_items.append({
                                "ì œëª©": title,
                                "ë§í¬": link,
                                "ì¶œì²˜": "Nnavi"  # Nnaviì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ì„ì„ í‘œì‹œ
                            })
        
        return news_items
        
    except Exception as e:
        print(f"âš ï¸ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return []

# ì˜¤ëŠ˜ ë‚ ì§œì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ëˆ„ì  ìˆ˜ì§‘
def fetch_all_pages_news(date=None):
    """ì§€ì •ëœ ë‚ ì§œì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ëˆ„ì í•˜ì—¬ ìˆ˜ì§‘"""
    if date is None:
        date = get_today_date()
    
    all_news_items = []
    seen_titles = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì œëª© ì§‘í•©
    
    # ë¨¼ì € í˜ì´ì§€ ê°œìˆ˜ íŒŒì•…
    page_count = get_today_page_count(date=date)
    
    if page_count == 0:
        # í˜ì´ì§€ ê°œìˆ˜ë¥¼ íŒŒì•…í•˜ì§€ ëª»í•œ ê²½ìš°, ì²« í˜ì´ì§€ë§Œ ìˆ˜ì§‘
        page_count = 1
    
    print(f"ğŸ“„ ì´ {page_count}í˜ì´ì§€ ìˆ˜ì§‘ ì‹œì‘...")
    
    # ê° í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ë°ì´í„° ìˆ˜ì§‘
    for page in range(1, page_count + 1):
        try:
            print(f"ğŸ“„ í˜ì´ì§€ {page}/{page_count} ìˆ˜ì§‘ ì¤‘...")
            
            # í˜ì´ì§€ë³„ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            page_news = fetch_news_list_from_page(date=date, page=page)
            
            # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
            for news in page_news:
                title = news.get("ì œëª©", "")
                if title and title not in seen_titles:
                    seen_titles.add(title)
                    news["í˜ì´ì§€"] = page  # ì–´ëŠ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì™”ëŠ”ì§€ í‘œì‹œ
                    all_news_items.append(news)
            
            # API í˜¸ì¶œ ì œí•œì„ ê³ ë ¤í•œ ì§§ì€ ëŒ€ê¸°
            time.sleep(0.5)
            
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    print(f"âœ… ì´ {len(all_news_items)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
    
    return all_news_items

# ìºì‹œë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
_summary_and_stocks_cache = {}

# OpenAIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ì œëª©ë“¤ ìš”ì•½ ë° ì¶”ì²œ ì¢…ëª© ì¶”ì¶œ (í•˜ë‚˜ì˜ API ìš”ì²­)
def get_summary_and_stocks_with_openai(titles: list) -> tuple[str, str]:
    """OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ì œëª©ë“¤ì„ ìš”ì•½í•˜ê³  ì¶”ì²œ ì¢…ëª© 10ê°œë¥¼ ì¶”ì¶œ (í•˜ë‚˜ì˜ API ìš”ì²­, ìºì‹± ì ìš©)"""
    if not openai_client:
        print("âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # ì œëª©ë“¤ì„ ê°„ë‹¨íˆ ê²°í•© (500ì ì œí•œ)
        combined = " | ".join(titles)
        summary = combined[:500] if len(combined) > 500 else combined
        return summary, ""
    
    if not titles:
        return "", ""
    
    # ìºì‹œ í‚¤ ìƒì„± (ì œëª©ë“¤ì˜ í•´ì‹œê°’)
    cache_key = hashlib.md5("|".join(titles[:10]).encode('utf-8')).hexdigest()
    
    # ìºì‹œì— ìˆìœ¼ë©´ ë°˜í™˜
    if cache_key in _summary_and_stocks_cache:
        print("ğŸ“‹ ìºì‹œì—ì„œ ìš”ì•½ ë° ì¶”ì²œ ì¢…ëª© ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
        cached_result = _summary_and_stocks_cache[cache_key]
        return cached_result["summary"], cached_result["topstock"]
    
    try:
        # ëª¨ë“  ì œëª©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        titles_text = "\n".join([f"- {title}" for title in titles[:50]])  # ìµœëŒ€ 50ê°œê¹Œì§€ë§Œ
        if len(titles) > 50:
            titles_text += f"\n... ì™¸ {len(titles) - 50}ê°œ ë‰´ìŠ¤"
        
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ë¶„ì„ ë° ì£¼ì‹ íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë‰´ìŠ¤ ì œëª©ë“¤ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:\n1. ë‰´ìŠ¤ ì œëª©ë“¤ì„ ì¢…í•©í•˜ì—¬ 500ì ì´ë‚´ë¡œ ìš”ì•½\n2. ë‰´ìŠ¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ íˆ¬ì ê°€ì¹˜ê°€ ë†’ì€ í•œêµ­ ì£¼ì‹ ì¢…ëª© 10ê°œë¥¼ ì¶”ì²œ\n\nì‘ë‹µ í˜•ì‹:\n[ìš”ì•½]\n(ì—¬ê¸°ì— 500ì ì´ë‚´ ìš”ì•½)\n\n[ì¶”ì²œì¢…ëª©]\nì¢…ëª©1, ì¢…ëª©2, ì¢…ëª©3, ì¢…ëª©4, ì¢…ëª©5, ì¢…ëª©6, ì¢…ëª©7, ì¢…ëª©8, ì¢…ëª©9, ì¢…ëª©10"
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{titles_text}\n\nìœ„ ë‰´ìŠ¤ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ:\n1. 500ì ì´ë‚´ë¡œ ìš”ì•½\n2. íˆ¬ì ê°€ì¹˜ê°€ ë†’ì€ í•œêµ­ ì£¼ì‹ ì¢…ëª© 10ê°œ ì¶”ì²œ (ì¢…ëª©ëª… ë˜ëŠ” ì¢…ëª©ì½”ë“œ 6ìë¦¬, ì‰¼í‘œë¡œ êµ¬ë¶„)\n\n[ìš”ì•½]ê³¼ [ì¶”ì²œì¢…ëª©] í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                }
            ],
            max_tokens=500,
            temperature=0.7
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # ê²°ê³¼ íŒŒì‹±
        summary = ""
        topstock = ""
        
        # [ìš”ì•½] ì„¹ì…˜ ì¶”ì¶œ
        if "[ìš”ì•½]" in result_text:
            summary_part = result_text.split("[ìš”ì•½]")[1]
            if "[ì¶”ì²œì¢…ëª©]" in summary_part:
                summary = summary_part.split("[ì¶”ì²œì¢…ëª©]")[0].strip()
            else:
                summary = summary_part.strip()
        else:
            # [ìš”ì•½] íƒœê·¸ê°€ ì—†ìœ¼ë©´ ì²« ë¶€ë¶„ì„ ìš”ì•½ìœ¼ë¡œ ê°„ì£¼
            if "[ì¶”ì²œì¢…ëª©]" in result_text:
                summary = result_text.split("[ì¶”ì²œì¢…ëª©]")[0].strip()
            else:
                summary = result_text[:500]
        
        # [ì¶”ì²œì¢…ëª©] ì„¹ì…˜ ì¶”ì¶œ
        if "[ì¶”ì²œì¢…ëª©]" in result_text:
            topstock_part = result_text.split("[ì¶”ì²œì¢…ëª©]")[1].strip()
            # ì²« ì¤„ë§Œ ê°€ì ¸ì˜¤ê¸° (ì¶”ê°€ ì„¤ëª… ì œê±°)
            topstock = topstock_part.split("\n")[0].strip()
        else:
            # [ì¶”ì²œì¢…ëª©] íƒœê·¸ê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ì¶”ì²œ ì¢…ëª©ìœ¼ë¡œ ê°„ì£¼
            lines = result_text.split("\n")
            for line in reversed(lines):
                if "," in line and len(line.strip()) > 10:
                    topstock = line.strip()
                    break
        
        # 500ì ì œí•œ
        if len(summary) > 500:
            summary = summary[:500]
        
        # 255ì ì œí•œ (VARCHAR(255))
        if len(topstock) > 255:
            topstock = topstock[:255]
        
        # ìºì‹œì— ì €ì¥
        _summary_and_stocks_cache[cache_key] = {
            "summary": summary,
            "topstock": topstock
        }
        print("ğŸ’¾ ìš”ì•½ ë° ì¶”ì²œ ì¢…ëª© ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        
        return summary, topstock
        
    except Exception as e:
        print(f"âš ï¸ OpenAI ìš”ì•½ ë° ì¶”ì²œ ì¢…ëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        combined = " | ".join(titles[:10])  # ìµœëŒ€ 10ê°œë§Œ
        if len(titles) > 10:
            combined += f" ... ì™¸ {len(titles) - 10}ê°œ"
        summary = combined[:500] if len(combined) > 500 else combined
        topstock = ""
        
        # ì‹¤íŒ¨í•œ ê²°ê³¼ë„ ìºì‹œì— ì €ì¥ (ì¬ì‹œë„ ë°©ì§€)
        _summary_and_stocks_cache[cache_key] = {
            "summary": summary,
            "topstock": topstock
        }
        
        return summary, topstock

# Supabaseì— ë‰´ìŠ¤ ë°ì´í„° ì €ì¥
def save_news_to_supabase(news_list: list, filename_without_ext: str) -> bool:
    """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Supabase daily_new í…Œì´ë¸”ì— ì €ì¥ (1ê°œì˜ ë ˆì½”ë“œë¡œ ì €ì¥)"""
    if not supabase_client:
        print("âš ï¸ Supabase ì—°ê²° ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    if not news_list:
        print("âš ï¸ ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        print(f"\nğŸ’¾ Supabaseì— {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ 1ê°œ ë ˆì½”ë“œë¡œ ì €ì¥ ì¤‘...")
        
        # ëª¨ë“  ë‰´ìŠ¤ ì œëª© ì¶”ì¶œ
        titles = [news.get("ì œëª©", "") for news in news_list if news.get("ì œëª©")]
        
        if not titles:
            print("âš ï¸ ì œëª©ì´ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“ ì´ {len(titles)}ê°œì˜ ë‰´ìŠ¤ ì œëª©ì„ ë¶„ì„ ì¤‘... (ìš”ì•½ + ì¶”ì²œ ì¢…ëª©)")
        
        # OpenAIë¡œ ëª¨ë“  ì œëª©ë“¤ì„ ì¢…í•©í•˜ì—¬ ìš”ì•½ ë° ì¶”ì²œ ì¢…ëª© ì¶”ì¶œ (í•˜ë‚˜ì˜ API ìš”ì²­)
        summary, topstock = get_summary_and_stocks_with_openai(titles)
        
        print(f"âœ… ìš”ì•½ ì™„ë£Œ: {len(summary)}ì")
        if topstock:
            print(f"âœ… ì¶”ì²œ ì¢…ëª© ì¶”ì¶œ ì™„ë£Œ: {topstock}")
        else:
            print(f"âš ï¸ ì¶”ì²œ ì¢…ëª© ì¶”ì¶œ ì‹¤íŒ¨")
        
        # ì €ì¥í•  ë°ì´í„° ì¤€ë¹„
        insert_data = {
            "content": news_list,  # ì „ì²´ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ JSONBë¡œ ì €ì¥
            "summary": summary,  # ëª¨ë“  ì œëª©ì„ ìš”ì•½í•œ ê²°ê³¼ (500ì)
            "cont_date": filename_without_ext,  # JSON íŒŒì¼ëª… (ì˜ˆ: "2026-01-28_01")
            "topstock": topstock  # ì¶”ì²œ ì¢…ëª© 10ê°œ
        }
        
        print(f"ğŸ“¤ ì €ì¥ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
        print(f"   - content: {len(news_list)}ê°œ ë‰´ìŠ¤")
        print(f"   - summary: {len(summary)}ì")
        print(f"   - cont_date: {filename_without_ext}")
        print(f"   - topstock: {topstock}")
        
        # Supabaseì— ë°ì´í„° ì‚½ì… (1ê°œì˜ ë ˆì½”ë“œ)
        result = supabase_client.table("daily_new").insert(insert_data).execute()
        
        # ê²°ê³¼ í™•ì¸
        if result.data:
            print(f"âœ… Supabase ì €ì¥ ì™„ë£Œ: 1ê°œ ë ˆì½”ë“œ ì €ì¥ ì„±ê³µ")
            print(f"   ì €ì¥ëœ ID: {result.data[0].get('id', 'N/A')}")
            return True
        else:
            print(f"âš ï¸ Supabase ì €ì¥ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return False
        
    except Exception as e:
        print(f"âŒ Supabase ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"   ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        import traceback
        print("\nìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        traceback.print_exc()
        return False

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ“° ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
    print("="*60)
    
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    data_dir = "data"
    today_date = get_today_date()
    print(f"ğŸ“… ë‚ ì§œ: {today_date}")
    
    # ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ (5ì¼ ì´ìƒ)
    print("\nğŸ—‘ï¸ ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    delete_old_files(data_dir, days=5)
    
    # ì˜¤ëŠ˜ ë‚ ì§œì˜ ëª¨ë“  í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    news_list = fetch_all_pages_news(date=today_date)
    
    if news_list:
        # ìƒˆ íŒŒì¼ëª… ìƒì„± (ë‚ ì§œ_ë²ˆí˜¸ í˜•ì‹)
        filepath = generate_filename(data_dir, today_date)
        filename_without_ext = os.path.basename(filepath).replace('.json', '')  # í™•ì¥ì ì œê±°
        print(f"ğŸ“ íŒŒì¼ëª…: {os.path.basename(filepath)}")
        
        # JSON íŒŒì¼ë¡œ ë°ì´í„° ì €ì¥
        if save_data_to_json(news_list, filepath):
            print(f"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ JSON íŒŒì¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        # Supabaseì— ë°ì´í„° ì €ì¥ (íŒŒì¼ëª… í˜•ì‹ìœ¼ë¡œ ì €ì¥: ì˜ˆ: "2026-01-28_01")
        if save_news_to_supabase(news_list, filename_without_ext):
            print(f"âœ… {len(news_list)}ê°œì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âš ï¸ Supabase ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    else:
        print("âš ï¸ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print("="*60 + "\n")

if __name__ == "__main__":
    main()
